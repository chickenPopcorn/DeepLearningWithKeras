{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"//fonts.googleapis.com/css?family=Quicksand:300\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"//fonts.googleapis.com/css?family=Quicksand:300\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "### Train Validate and Test\n",
    "\n",
    "- **Training**: The sample of data used to fit the model.\n",
    "The actual dataset that we use to train the model. The model sees and learns from this data.\n",
    "\n",
    "- **Validation**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. \n",
    "\n",
    "\n",
    "- **Test Dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. It is only used once a model is completely trained(using the train and validation sets).\n",
    "\n",
    "![half center](images/train_validate_test.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Overfitting vs Underfitting\n",
    "\n",
    "### Optimization and Generalization. \n",
    "\n",
    "**Optimization** \n",
    "- the process of adjusting a model to get the best performance possible on the training data\n",
    "\n",
    "**Generalization**\n",
    "- how well the trained model would perform on data it has never seen before. \n",
    "\n",
    "The goal of the game is to get good generalization, butone can only adjust the model based on its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Overfitting vs Underfitting\n",
    "\n",
    "![half center](images/fit_graph.jpg)\n",
    "\n",
    "\n",
    "At the beginning of training, optimization and generalization are correlated: \n",
    "- the lower your loss on training data, the lower your loss on test data. \n",
    "\n",
    "While this is happening, your model is said to be under-fit: \n",
    "there is still progress to be made; the network hasn't yet modeled all relevant patterns in the training data. \n",
    "\n",
    "But after a certain number of iterations(epochs) on the training data, generalization stops improving, validation metrics stall then start degrading: the model is then starting to over-fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Overfitting vs Underfitting\n",
    "\n",
    "![half center](images/overfitting_underfitting.png)\n",
    "\n",
    "To prevent a model from learning misleading or irrelevant patterns found in the training data, \n",
    "- More training data: A model trained on more data will naturally generalize better. \n",
    "- Modulate/constraint the information model stores: If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Overfitting vs Underfitting\n",
    "\n",
    "To prevent a model from learning misleading or irrelevant patterns found in the training data, \n",
    "- More training data: A model trained on more data will naturally generalize better. \n",
    "- Modulate/constraint the information model stores: If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "- Reducing the network's size\n",
    "- Adding weight regularization\n",
    "- Adding dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "### Adding weight regularization\n",
    "\n",
    "adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n",
    "\n",
    "- L1 regularization: the cost added is proportional to the absolute value of the weights coefficients\n",
    "- L2 regularization: the cost added is proportional to the square of the value of the weights coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "### Adding dropout\n",
    "\n",
    "- **Dropout**: applied to a layer, consists of randomly \"dropping out\" (i.e. setting to zero) a number of output features of the layer during training. \n",
    "\n",
    "![half center](images/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "\n",
    "By normalizing all of our inputs to a standard scale, we're allowing the network to more quickly learn the optimal parameters for each input node.\n",
    "\n",
    "![half center](images/mean_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Next: basics NN with Keras.ipynb](03_01_basics_of_keras_NN.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
